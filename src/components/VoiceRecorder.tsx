'use client'

import { useState, useRef, useEffect } from 'react'

interface VoiceAnalysis {
  clarity: number // 1-10ç‚¹
  volume: number // 1-5ç‚¹ (1:å°ã•ã™ãã‚‹, 2:ã‚„ã‚„å°ã•ã„, 3:é©åˆ‡, 4:ã‚„ã‚„å¤§ãã„, 5:å¤§ãã™ãã‚‹)
  speechRate: number // æ–‡å­—/åˆ†
  stability: number // 1-10ç‚¹ (å£°ã®å®‰å®šæ€§)
}

interface VoiceRecorderProps {
  onTranscriptChange: (transcript: string) => void
  onRecordingStateChange: (isRecording: boolean) => void
  onVoiceAnalysis?: (analysis: VoiceAnalysis) => void
  onClear?: () => void
}

export default function VoiceRecorder({ onTranscriptChange, onRecordingStateChange, onVoiceAnalysis, onClear }: VoiceRecorderProps) {
  const [isRecording, setIsRecording] = useState(false)
  const [isPaused, setIsPaused] = useState(false)
  const [transcript, setTranscript] = useState('')
  const [isSupported, setIsSupported] = useState(false)
  const [error, setError] = useState<string | null>(null)
  const [recordingTime, setRecordingTime] = useState(0)
  const [waveformData, setWaveformData] = useState<number[]>(new Array(50).fill(0))
  const [audioBlob, setAudioBlob] = useState<Blob | null>(null)
  const [isPlaying, setIsPlaying] = useState(false)

  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const recognitionRef = useRef<SpeechRecognition | null>(null)
  const chunksRef = useRef<Blob[]>([])
  const transcriptRef = useRef('')
  const audioContextRef = useRef<AudioContext | null>(null)
  const analyserRef = useRef<AnalyserNode | null>(null)
  const dataArrayRef = useRef<Uint8Array | null>(null)
  const startTimeRef = useRef<number>(0)
  const volumeHistoryRef = useRef<number[]>([])
  const analysisIntervalRef = useRef<NodeJS.Timeout | null>(null)
  const timerIntervalRef = useRef<NodeJS.Timeout | null>(null)
  const waveformIntervalRef = useRef<NodeJS.Timeout | null>(null)
  const audioRef = useRef<HTMLAudioElement | null>(null)

  const initializeSpeechRecognition = () => {
    if (typeof window === 'undefined') return null
    
    const SpeechRecognitionClass = window.SpeechRecognition || window.webkitSpeechRecognition
    if (!SpeechRecognitionClass) return null
    
    const recognition = new SpeechRecognitionClass()
    
    recognition.continuous = true
    recognition.interimResults = true
    recognition.lang = 'ja-JP'

    recognition.onresult = (event: SpeechRecognitionEvent) => {
      let finalTranscript = ''
      let interimTranscript = ''

      for (let i = event.resultIndex; i < event.results.length; i++) {
        const resultTranscript = event.results[i][0].transcript
        if (event.results[i].isFinal) {
          finalTranscript += resultTranscript
        } else {
          interimTranscript += resultTranscript
        }
      }

      if (finalTranscript) {
        transcriptRef.current += finalTranscript
        setTranscript(transcriptRef.current)
        onTranscriptChange(transcriptRef.current + interimTranscript)
      } else {
        onTranscriptChange(transcriptRef.current + interimTranscript)
      }
    }

    recognition.onerror = (event: SpeechRecognitionErrorEvent) => {
      console.error('Speech recognition error:', event.error)
      setError(`éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: ${event.error}`)
    }

    recognition.onend = () => {
      if (isRecording) {
        setTimeout(() => {
          if (recognitionRef.current && isRecording) {
            recognitionRef.current.start()
          }
        }, 100)
      }
    }

    return recognition
  }

  const analyzeAudio = () => {
    if (!analyserRef.current || !dataArrayRef.current) return

    analyserRef.current.getByteFrequencyData(dataArrayRef.current)
    
    // éŸ³é‡è¨ˆç®—
    let sum = 0
    for (let i = 0; i < dataArrayRef.current.length; i++) {
      sum += dataArrayRef.current[i]
    }
    const averageVolume = sum / dataArrayRef.current.length
    volumeHistoryRef.current.push(averageVolume)
    
    // å±¥æ­´ã‚’æœ€å¤§500å€‹ã«åˆ¶é™
    if (volumeHistoryRef.current.length > 500) {
      volumeHistoryRef.current.shift()
    }
  }

  const updateWaveform = () => {
    if (!analyserRef.current || !dataArrayRef.current) return

    analyserRef.current.getByteFrequencyData(dataArrayRef.current)
    
    // æ³¢å½¢ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ï¼ˆ50å€‹ã®ãƒãƒ¼ã§è¡¨ç¤ºï¼‰
    const newWaveformData = []
    const barCount = 50
    const dataPerBar = Math.floor(dataArrayRef.current.length / barCount)
    
    for (let i = 0; i < barCount; i++) {
      let sum = 0
      for (let j = 0; j < dataPerBar; j++) {
        sum += dataArrayRef.current[i * dataPerBar + j]
      }
      newWaveformData.push(sum / dataPerBar / 255) // 0-1ã®ç¯„å›²ã«æ­£è¦åŒ–
    }
    
    setWaveformData(newWaveformData)
  }

  const formatTime = (seconds: number) => {
    const mins = Math.floor(seconds / 60)
    const secs = seconds % 60
    return `${mins}:${secs.toString().padStart(2, '0')}`
  }

  const calculateVoiceAnalysis = (): VoiceAnalysis => {
    const transcript = transcriptRef.current
    const recordingDurationSeconds = startTimeRef.current > 0 ? (Date.now() - startTimeRef.current) / 1000 : 60 // ç§’
    const recordingDurationMinutes = Math.max(recordingDurationSeconds / 60, 0.1) // æœ€ä½0.1åˆ†ã¨ã—ã¦è¨ˆç®—
    const volumeHistory = volumeHistoryRef.current

    // è©±é€Ÿåˆ†æ (æ–‡å­—æ•°/åˆ†)
    const speechRate = transcript.length > 0 ? Math.round(transcript.length / recordingDurationMinutes) : 0

    // éŸ³é‡åˆ†æ (1-5ç‚¹)
    const averageVolume = volumeHistory.length > 0 
      ? volumeHistory.reduce((a, b) => a + b, 0) / volumeHistory.length 
      : 0
    
    let volume = 3 // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯é©åˆ‡
    if (averageVolume < 30) volume = 1 // å°ã•ã™ãã‚‹
    else if (averageVolume < 60) volume = 2 // ã‚„ã‚„å°ã•ã„
    else if (averageVolume < 120) volume = 3 // é©åˆ‡
    else if (averageVolume < 180) volume = 4 // ã‚„ã‚„å¤§ãã„
    else volume = 5 // å¤§ãã™ãã‚‹

    // å£°ã®å®‰å®šæ€§ (1-10ç‚¹) - éŸ³é‡ã®å¤‰å‹•ã‹ã‚‰è¨ˆç®—
    let stability = 8 // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    if (volumeHistory.length > 10) {
      const variance = volumeHistory.reduce((acc, vol, index, arr) => {
        const avg = arr.reduce((a, b) => a + b, 0) / arr.length
        return acc + Math.pow(vol - avg, 2)
      }, 0) / volumeHistory.length
      
      const stdDev = Math.sqrt(variance)
      
      if (stdDev < 10) stability = 9
      else if (stdDev < 20) stability = 8
      else if (stdDev < 30) stability = 7
      else if (stdDev < 40) stability = 6
      else if (stdDev < 50) stability = 5
      else stability = 4
    }

    // æ˜ç­åº¦ (1-10ç‚¹) - è©±é€Ÿã¨éŸ³é‡ã‹ã‚‰æ¨å®š
    let clarity = 7 // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    const optimalSpeechRate = 350 // é©åˆ‡ãªè©±é€Ÿ
    const speechRateDiff = Math.abs(speechRate - optimalSpeechRate)
    
    if (speechRateDiff < 50 && volume === 3) clarity = 9
    else if (speechRateDiff < 100 && volume >= 2 && volume <= 4) clarity = 8
    else if (speechRateDiff < 150) clarity = 7
    else if (speechRateDiff < 200) clarity = 6
    else clarity = 5

    return {
      clarity,
      volume,
      speechRate,
      stability
    }
  }

  useEffect(() => {
    if (typeof window === 'undefined') return
    
    const hasMediaRecorder = 'mediaDevices' in navigator && 'MediaRecorder' in window
    const hasSpeechRecognition = 'webkitSpeechRecognition' in window || 'SpeechRecognition' in window
    
    setIsSupported(hasMediaRecorder && hasSpeechRecognition)

    if (hasSpeechRecognition) {
      recognitionRef.current = initializeSpeechRecognition()
    }

    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.stop()
      }
      if (mediaRecorderRef.current) {
        mediaRecorderRef.current.stop()
      }
    }
  }, [])

  const startRecording = async () => {
    try {
      setError(null)
      setRecordingTime(0)
      startTimeRef.current = Date.now()
      volumeHistoryRef.current = []
      
      // ãƒã‚¤ã‚¯ã‚¢ã‚¯ã‚»ã‚¹è¨±å¯ã‚’å–å¾—
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          sampleRate: 44100
        } 
      })

      // Web Audio API ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
      const audioContext = new AudioContext()
      const analyser = audioContext.createAnalyser()
      const source = audioContext.createMediaStreamSource(stream)
      
      analyser.fftSize = 2048
      analyser.smoothingTimeConstant = 0.8
      source.connect(analyser)
      
      audioContextRef.current = audioContext
      analyserRef.current = analyser
      dataArrayRef.current = new Uint8Array(analyser.frequencyBinCount)
      
      // å®šæœŸçš„ãªéŸ³å£°åˆ†æé–‹å§‹
      analysisIntervalRef.current = setInterval(analyzeAudio, 100)
      
      // æ³¢å½¢æ›´æ–°é–‹å§‹
      waveformIntervalRef.current = setInterval(updateWaveform, 50)
      
      // ã‚¿ã‚¤ãƒãƒ¼é–‹å§‹
      timerIntervalRef.current = setInterval(() => {
        setRecordingTime(prev => {
          const newTime = prev + 1
          // 5åˆ†ï¼ˆ300ç§’ï¼‰ã§è‡ªå‹•åœæ­¢
          if (newTime >= 300) {
            stopRecording()
          }
          return newTime
        })
      }, 1000)

      // MediaRecorder ã®è¨­å®š
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus'
      })

      chunksRef.current = []
      
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          chunksRef.current.push(event.data)
        }
      }

      mediaRecorder.onstop = () => {
        const audioBlob = new Blob(chunksRef.current, { type: 'audio/webm' })
        console.log('éŒ²éŸ³å®Œäº†:', audioBlob)
        setAudioBlob(audioBlob)
        
        // éŸ³å£°åˆ†æçµæœã‚’è¨ˆç®—
        const voiceAnalysis = calculateVoiceAnalysis()
        if (onVoiceAnalysis) {
          onVoiceAnalysis(voiceAnalysis)
        }
        
        // ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’åœæ­¢
        stream.getTracks().forEach(track => track.stop())
        
        // ã™ã¹ã¦ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«ã‚’ã‚¯ãƒªã‚¢
        clearAllIntervals()
      }

      mediaRecorderRef.current = mediaRecorder
      mediaRecorder.start(1000) // 1ç§’ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

      // éŸ³å£°èªè­˜é–‹å§‹
      if (recognitionRef.current) {
        recognitionRef.current.start()
      }

      setIsRecording(true)
      setIsPaused(false)
      onRecordingStateChange(true)

    } catch (err) {
      console.error('éŒ²éŸ³é–‹å§‹ã‚¨ãƒ©ãƒ¼:', err)
      setError('ãƒã‚¤ã‚¯ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹è¨±å¯ãŒå¿…è¦ã§ã™')
    }
  }

  const pauseRecording = () => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.pause()
    }
    
    if (recognitionRef.current) {
      recognitionRef.current.stop()
    }
    
    // ã‚¿ã‚¤ãƒãƒ¼ã¨æ³¢å½¢æ›´æ–°ã‚’åœæ­¢
    if (timerIntervalRef.current) {
      clearInterval(timerIntervalRef.current)
      timerIntervalRef.current = null
    }
    if (waveformIntervalRef.current) {
      clearInterval(waveformIntervalRef.current)
      waveformIntervalRef.current = null
    }
    
    setIsPaused(true)
    setWaveformData(new Array(50).fill(0)) // æ³¢å½¢ã‚’ã‚¯ãƒªã‚¢
  }

  const resumeRecording = () => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'paused') {
      mediaRecorderRef.current.resume()
    }
    
    // éŸ³å£°èªè­˜ã‚’å†é–‹
    if (recognitionRef.current) {
      recognitionRef.current.start()
    }
    
    // ã‚¿ã‚¤ãƒãƒ¼ã¨æ³¢å½¢æ›´æ–°ã‚’å†é–‹
    timerIntervalRef.current = setInterval(() => {
      setRecordingTime(prev => {
        const newTime = prev + 1
        if (newTime >= 300) {
          stopRecording()
        }
        return newTime
      })
    }, 1000)
    
    waveformIntervalRef.current = setInterval(updateWaveform, 50)
    
    setIsPaused(false)
  }

  const clearAllIntervals = () => {
    if (analysisIntervalRef.current) {
      clearInterval(analysisIntervalRef.current)
      analysisIntervalRef.current = null
    }
    if (timerIntervalRef.current) {
      clearInterval(timerIntervalRef.current)
      timerIntervalRef.current = null
    }
    if (waveformIntervalRef.current) {
      clearInterval(waveformIntervalRef.current)
      waveformIntervalRef.current = null
    }
    
    // AudioContextã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    if (audioContextRef.current) {
      audioContextRef.current.close()
      audioContextRef.current = null
    }
  }

  const stopRecording = () => {
    if (mediaRecorderRef.current) {
      if (mediaRecorderRef.current.state === 'recording' || mediaRecorderRef.current.state === 'paused') {
        mediaRecorderRef.current.stop()
      }
    }

    if (recognitionRef.current) {
      recognitionRef.current.stop()
    }

    setIsRecording(false)
    setIsPaused(false)
    setWaveformData(new Array(50).fill(0))
    onRecordingStateChange(false)
    
    clearAllIntervals()
  }


  const clearTranscript = () => {
    transcriptRef.current = ''
    setTranscript('')
    onTranscriptChange('')
    setError(null)
    setRecordingTime(0)
    setWaveformData(new Array(50).fill(0))
    setAudioBlob(null)
    chunksRef.current = []
    
    // éŸ³å£°èªè­˜ã‚’ãƒªã‚»ãƒƒãƒˆ
    if (recognitionRef.current) {
      recognitionRef.current.stop()
      recognitionRef.current = initializeSpeechRecognition()
    }
    
    // ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã®çŠ¶æ…‹ã‚‚ãƒªã‚»ãƒƒãƒˆ
    if (onClear) {
      onClear()
    }
  }

  const playRecording = () => {
    if (audioBlob && !isPlaying) {
      const audioUrl = URL.createObjectURL(audioBlob)
      const audio = new Audio(audioUrl)
      audioRef.current = audio
      
      audio.onplay = () => setIsPlaying(true)
      audio.onended = () => {
        setIsPlaying(false)
        URL.revokeObjectURL(audioUrl)
      }
      audio.onerror = () => {
        setIsPlaying(false)
        URL.revokeObjectURL(audioUrl)
        setError('éŸ³å£°ã®å†ç”Ÿã«å¤±æ•—ã—ã¾ã—ãŸ')
      }
      
      audio.play().catch(err => {
        setIsPlaying(false)
        URL.revokeObjectURL(audioUrl)
        setError('éŸ³å£°ã®å†ç”Ÿã«å¤±æ•—ã—ã¾ã—ãŸ')
      })
    } else if (isPlaying && audioRef.current) {
      audioRef.current.pause()
      setIsPlaying(false)
    }
  }

  if (!isSupported) {
    return (
      <div className="bg-red-50 border border-red-200 rounded-lg p-4">
        <p className="text-red-700">
          ãŠä½¿ã„ã®ãƒ–ãƒ©ã‚¦ã‚¶ã¯éŸ³å£°éŒ²éŸ³æ©Ÿèƒ½ã«å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“ã€‚
          <br />
          Chromeã€Firefoxã€Safari ã®æœ€æ–°ç‰ˆã‚’ã”åˆ©ç”¨ãã ã•ã„ã€‚
        </p>
      </div>
    )
  }

  return (
    <div className="space-y-4">
      {/* éŒ²éŸ³æ™‚é–“ã¨ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ */}
      {(isRecording || isPaused || recordingTime > 0) && (
        <div className="text-center space-y-2">
          <div className="text-2xl font-mono text-gray-800">
            {formatTime(recordingTime)}
            {recordingTime >= 300 && <span className="text-red-500 ml-2">æœ€å¤§æ™‚é–“ã«åˆ°é”</span>}
          </div>
          <div className="text-sm text-gray-600">
            {isRecording && !isPaused && 'ğŸ”´ éŒ²éŸ³ä¸­...'}
            {isPaused && 'â¸ï¸ ä¸€æ™‚åœæ­¢ä¸­'}
            {!isRecording && recordingTime > 0 && 'â¹ï¸ éŒ²éŸ³å®Œäº†'}
          </div>
        </div>
      )}

      {/* éŸ³å£°æ³¢å½¢è¡¨ç¤º */}
      {(isRecording || isPaused) && (
        <div className="bg-gray-900 rounded-lg p-4">
          <div className="flex items-end justify-center space-x-1 h-20">
            {waveformData.map((amplitude, index) => (
              <div
                key={index}
                className="bg-blue-400 rounded-t transition-all duration-75"
                style={{
                  height: `${Math.max(2, amplitude * 60)}px`,
                  width: '3px'
                }}
              />
            ))}
          </div>
        </div>
      )}

      {/* éŒ²éŸ³ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ« */}
      <div className="text-center space-y-3">
        {!isRecording && !isPaused && !audioBlob ? (
          // éŒ²éŸ³é–‹å§‹çŠ¶æ…‹
          <button
            onClick={startRecording}
            disabled={!isSupported}
            className="px-8 py-4 rounded-full font-medium transition-all duration-200 bg-blue-600 text-white hover:bg-blue-700 shadow-md hover:shadow-lg disabled:opacity-50 disabled:cursor-not-allowed"
          >
            ğŸ¤ éŒ²éŸ³é–‹å§‹
          </button>
        ) : !isRecording && !isPaused && audioBlob ? (
          // éŒ²éŸ³å®Œäº†çŠ¶æ…‹ - å†éŒ²éŸ³ãƒ»å†ç”Ÿãƒœã‚¿ãƒ³
          <div className="flex gap-3 justify-center">
            <button
              onClick={clearTranscript}
              className="px-6 py-3 rounded-md font-medium transition-all duration-200 bg-gray-600 text-white hover:bg-gray-700"
            >
              ğŸ”„ å†éŒ²éŸ³
            </button>
            <button
              onClick={playRecording}
              disabled={!audioBlob}
              className="px-6 py-3 rounded-md font-medium transition-all duration-200 bg-green-600 text-white hover:bg-green-700 disabled:opacity-50 disabled:cursor-not-allowed"
            >
              {isPlaying ? 'â¸ï¸ åœæ­¢' : 'â–¶ï¸ å†ç”Ÿ'}
            </button>
          </div>
        ) : (
          // éŒ²éŸ³ä¸­ãƒ»ä¸€æ™‚åœæ­¢ä¸­ã®åˆ¶å¾¡
          <div className="flex gap-3 justify-center">
            {isRecording && !isPaused && (
              <>
                <button
                  onClick={pauseRecording}
                  className="px-6 py-3 rounded-md font-medium transition-all duration-200 bg-yellow-600 text-white hover:bg-yellow-700"
                >
                  â¸ï¸ ä¸€æ™‚åœæ­¢
                </button>
                <button
                  onClick={stopRecording}
                  className="px-6 py-3 rounded-md font-medium transition-all duration-200 bg-red-600 text-white hover:bg-red-700"
                >
                  â¹ï¸ åœæ­¢
                </button>
              </>
            )}
            
            {isPaused && (
              <>
                <button
                  onClick={resumeRecording}
                  className="px-6 py-3 rounded-md font-medium transition-all duration-200 bg-green-600 text-white hover:bg-green-700"
                >
                  â–¶ï¸ å†é–‹
                </button>
                <button
                  onClick={stopRecording}
                  className="px-6 py-3 rounded-md font-medium transition-all duration-200 bg-red-600 text-white hover:bg-red-700"
                >
                  â¹ï¸ åœæ­¢
                </button>
              </>
            )}
          </div>
        )}
        
      </div>

      {/* ã‚¨ãƒ©ãƒ¼è¡¨ç¤º */}
      {error && (
        <div className="bg-red-50 border border-red-200 rounded-lg p-3">
          <p className="text-red-700 text-sm">{error}</p>
        </div>
      )}

      {/* æ–‡å­—èµ·ã“ã—çµæœ */}
      {transcript && (
        <div className="space-y-2">
          <div className="flex justify-between items-center">
            <h4 className="font-medium text-gray-700">æ–‡å­—èµ·ã“ã—çµæœ</h4>
            {!audioBlob && (
              <button
                onClick={clearTranscript}
                className="text-sm text-gray-500 hover:text-gray-700 underline"
              >
                ã‚¯ãƒªã‚¢
              </button>
            )}
          </div>
          <textarea
            value={transcript}
            onChange={(e) => {
              setTranscript(e.target.value)
              onTranscriptChange(e.target.value)
            }}
            className="w-full p-4 border rounded-lg h-32 resize-none focus:outline-none focus:ring-2 focus:ring-blue-500 focus:border-transparent"
            placeholder="æ–‡å­—èµ·ã“ã—çµæœãŒã“ã“ã«è¡¨ç¤ºã•ã‚Œã¾ã™..."
          />
          <p className="text-sm text-gray-500 text-right">
            æ–‡å­—æ•°: {transcript.length} æ–‡å­—
          </p>
        </div>
      )}
    </div>
  )
}

// TypeScript ã®å‹å®šç¾©ã‚’è¿½åŠ 
declare global {
  interface Window {
    SpeechRecognition: any
    webkitSpeechRecognition: any
  }
}

interface SpeechRecognition extends EventTarget {
  continuous: boolean
  interimResults: boolean
  lang: string
  start(): void
  stop(): void
  onresult: (event: SpeechRecognitionEvent) => void
  onerror: (event: SpeechRecognitionErrorEvent) => void
  onend: () => void
}

interface SpeechRecognitionEvent {
  resultIndex: number
  results: SpeechRecognitionResultList
}

interface SpeechRecognitionResultList {
  length: number
  item(index: number): SpeechRecognitionResult
  [index: number]: SpeechRecognitionResult
}

interface SpeechRecognitionResult {
  length: number
  item(index: number): SpeechRecognitionAlternative
  [index: number]: SpeechRecognitionAlternative
  isFinal: boolean
}

interface SpeechRecognitionAlternative {
  transcript: string
  confidence: number
}

interface SpeechRecognitionErrorEvent {
  error: string
  message: string
}