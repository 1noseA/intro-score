'use client'

import { useState, useRef, useEffect } from 'react'

interface VoiceRecorderProps {
  onTranscriptChange: (transcript: string) => void
  onRecordingStateChange: (isRecording: boolean) => void
}

export default function VoiceRecorder({ onTranscriptChange, onRecordingStateChange }: VoiceRecorderProps) {
  const [isRecording, setIsRecording] = useState(false)
  const [transcript, setTranscript] = useState('')
  const [isSupported, setIsSupported] = useState(false)
  const [error, setError] = useState<string | null>(null)

  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const recognitionRef = useRef<SpeechRecognition | null>(null)
  const chunksRef = useRef<Blob[]>([])

  useEffect(() => {
    // ãƒ–ãƒ©ã‚¦ã‚¶ã‚µãƒãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯ï¼ˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚µã‚¤ãƒ‰ã®ã¿å®Ÿè¡Œï¼‰
    if (typeof window === 'undefined') return
    
    const hasMediaRecorder = 'mediaDevices' in navigator && 'MediaRecorder' in window
    const hasSpeechRecognition = 'webkitSpeechRecognition' in window || 'SpeechRecognition' in window
    
    setIsSupported(hasMediaRecorder && hasSpeechRecognition)

    if (hasSpeechRecognition) {
      // Speech Recognition ã®åˆæœŸåŒ–
      const SpeechRecognitionClass = window.SpeechRecognition || window.webkitSpeechRecognition
      const recognition = new SpeechRecognitionClass()
      
      recognition.continuous = true
      recognition.interimResults = true
      recognition.lang = 'ja-JP'

      recognition.onresult = (event: SpeechRecognitionEvent) => {
        let finalTranscript = ''
        let interimTranscript = ''

        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript
          if (event.results[i].isFinal) {
            finalTranscript += transcript
          } else {
            interimTranscript += transcript
          }
        }

        const fullTranscript = transcript + finalTranscript
        setTranscript(fullTranscript)
        onTranscriptChange(fullTranscript + interimTranscript)
      }

      recognition.onerror = (event: SpeechRecognitionErrorEvent) => {
        console.error('Speech recognition error:', event.error)
        setError(`éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: ${event.error}`)
      }

      recognition.onend = () => {
        if (isRecording) {
          // éŒ²éŸ³ä¸­ã«éŸ³å£°èªè­˜ãŒåœæ­¢ã—ãŸå ´åˆã¯å†é–‹
          recognition.start()
        }
      }

      recognitionRef.current = recognition
    }

    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.stop()
      }
      if (mediaRecorderRef.current) {
        mediaRecorderRef.current.stop()
      }
    }
  }, [transcript, onTranscriptChange, isRecording])

  const startRecording = async () => {
    try {
      setError(null)
      
      // ãƒã‚¤ã‚¯ã‚¢ã‚¯ã‚»ã‚¹è¨±å¯ã‚’å–å¾—
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          sampleRate: 44100
        } 
      })

      // MediaRecorder ã®è¨­å®š
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus'
      })

      chunksRef.current = []
      
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          chunksRef.current.push(event.data)
        }
      }

      mediaRecorder.onstop = () => {
        const audioBlob = new Blob(chunksRef.current, { type: 'audio/webm' })
        // å¿…è¦ã«å¿œã˜ã¦éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
        console.log('éŒ²éŸ³å®Œäº†:', audioBlob)
        
        // ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’åœæ­¢
        stream.getTracks().forEach(track => track.stop())
      }

      mediaRecorderRef.current = mediaRecorder
      mediaRecorder.start(1000) // 1ç§’ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

      // éŸ³å£°èªè­˜é–‹å§‹
      if (recognitionRef.current) {
        recognitionRef.current.start()
      }

      setIsRecording(true)
      onRecordingStateChange(true)

    } catch (err) {
      console.error('éŒ²éŸ³é–‹å§‹ã‚¨ãƒ©ãƒ¼:', err)
      setError('ãƒã‚¤ã‚¯ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹è¨±å¯ãŒå¿…è¦ã§ã™')
    }
  }

  const stopRecording = () => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop()
    }

    if (recognitionRef.current) {
      recognitionRef.current.stop()
    }

    setIsRecording(false)
    onRecordingStateChange(false)
  }

  const clearTranscript = () => {
    setTranscript('')
    onTranscriptChange('')
    setError(null)
  }

  if (!isSupported) {
    return (
      <div className="bg-red-50 border border-red-200 rounded-lg p-4">
        <p className="text-red-700">
          ãŠä½¿ã„ã®ãƒ–ãƒ©ã‚¦ã‚¶ã¯éŸ³å£°éŒ²éŸ³æ©Ÿèƒ½ã«å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“ã€‚
          <br />
          Chromeã€Firefoxã€Safari ã®æœ€æ–°ç‰ˆã‚’ã”åˆ©ç”¨ãã ã•ã„ã€‚
        </p>
      </div>
    )
  }

  return (
    <div className="space-y-4">
      {/* éŒ²éŸ³ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ« */}
      <div className="text-center">
        <button
          onClick={isRecording ? stopRecording : startRecording}
          disabled={!isSupported}
          className={`px-8 py-4 rounded-full font-medium transition-all duration-200 ${
            isRecording 
              ? 'bg-red-500 text-white hover:bg-red-600 shadow-lg transform scale-105' 
              : 'bg-blue-600 text-white hover:bg-blue-700 shadow-md hover:shadow-lg'
          } disabled:opacity-50 disabled:cursor-not-allowed`}
        >
          {isRecording ? (
            <>
              <span className="inline-block w-3 h-3 bg-white rounded-full mr-2 animate-pulse"></span>
              ğŸ”´ éŒ²éŸ³åœæ­¢
            </>
          ) : (
            'ğŸ¤ éŒ²éŸ³é–‹å§‹'
          )}
        </button>
        
        {isRecording && (
          <p className="text-sm text-gray-600 mt-2">
            éŒ²éŸ³ä¸­... è‡ªå·±ç´¹ä»‹ã‚’ãŠè©±ã—ãã ã•ã„
          </p>
        )}
      </div>

      {/* ã‚¨ãƒ©ãƒ¼è¡¨ç¤º */}
      {error && (
        <div className="bg-red-50 border border-red-200 rounded-lg p-3">
          <p className="text-red-700 text-sm">{error}</p>
        </div>
      )}

      {/* æ–‡å­—èµ·ã“ã—çµæœ */}
      {transcript && (
        <div className="space-y-2">
          <div className="flex justify-between items-center">
            <h4 className="font-medium text-gray-700">æ–‡å­—èµ·ã“ã—çµæœ</h4>
            <button
              onClick={clearTranscript}
              className="text-sm text-gray-500 hover:text-gray-700 underline"
            >
              ã‚¯ãƒªã‚¢
            </button>
          </div>
          <textarea
            value={transcript}
            onChange={(e) => {
              setTranscript(e.target.value)
              onTranscriptChange(e.target.value)
            }}
            className="w-full p-4 border rounded-lg h-32 resize-none focus:outline-none focus:ring-2 focus:ring-blue-500 focus:border-transparent"
            placeholder="æ–‡å­—èµ·ã“ã—çµæœãŒã“ã“ã«è¡¨ç¤ºã•ã‚Œã¾ã™..."
          />
          <p className="text-sm text-gray-500">
            æ–‡å­—æ•°: {transcript.length} æ–‡å­—
          </p>
        </div>
      )}
    </div>
  )
}

// TypeScript ã®å‹å®šç¾©ã‚’è¿½åŠ 
declare global {
  interface Window {
    SpeechRecognition: any
    webkitSpeechRecognition: any
  }
}

interface SpeechRecognition extends EventTarget {
  continuous: boolean
  interimResults: boolean
  lang: string
  start(): void
  stop(): void
  onresult: (event: SpeechRecognitionEvent) => void
  onerror: (event: SpeechRecognitionErrorEvent) => void
  onend: () => void
}

interface SpeechRecognitionEvent {
  resultIndex: number
  results: SpeechRecognitionResultList
}

interface SpeechRecognitionResultList {
  length: number
  item(index: number): SpeechRecognitionResult
  [index: number]: SpeechRecognitionResult
}

interface SpeechRecognitionResult {
  length: number
  item(index: number): SpeechRecognitionAlternative
  [index: number]: SpeechRecognitionAlternative
  isFinal: boolean
}

interface SpeechRecognitionAlternative {
  transcript: string
  confidence: number
}

interface SpeechRecognitionErrorEvent {
  error: string
  message: string
}